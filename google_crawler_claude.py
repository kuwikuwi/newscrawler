from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import requests
import pandas as pd
import re
from urllib.parse import quote, urlparse, unquote
import urllib3
import os
import time
import random

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

'''
< ê°œì„ ëœ Google News RSS í¬ë¡¤ëŸ¬ >
- RSS í”¼ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ 429 ì˜¤ë¥˜ ë°©ì§€
- ë‹¤ì¤‘ RSS ì†ŒìŠ¤ í™œìš©
- ë” ë‚˜ì€ ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
- í•œêµ­ì–´/ì˜ì–´ ì§€ì›
'''

class GoogleNewsRSSCrawler:
    def __init__(self, result_path='C:/Users/kibae/Desktop/google_news_crawling/result'):
        self.result_path = result_path
        self.results = []
        
        # ì•ˆì •ì ì¸ í—¤ë” ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/rss+xml, application/xml, text/xml, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }

    def parse_rss_date(self, date_str):
        """RSS ë‚ ì§œë¥¼ í•œêµ­ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            # RSS í‘œì¤€ ë‚ ì§œ í˜•ì‹: 'Mon, 30 May 2025 10:30:00 GMT'
            formats = [
                '%a, %d %b %Y %H:%M:%S %Z',
                '%a, %d %b %Y %H:%M:%S',
                '%d %b %Y %H:%M:%S',
                '%Y-%m-%dT%H:%M:%SZ',
                '%Y-%m-%d %H:%M:%S'
            ]
            
            for fmt in formats:
                try:
                    date_obj = datetime.strptime(date_str.strip(), fmt)
                    return date_obj.strftime('%Y.%m.%d.')
                except ValueError:
                    continue
                    
        except Exception as e:
            print(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e} - ì…ë ¥: {date_str}")
        
        return datetime.now().strftime('%Y.%m.%d.')

    def extract_source_from_title(self, title):
        """ì œëª©ì—ì„œ ì¶œì²˜ ì¶”ì¶œ"""
        # Google NewsëŠ” ì¢…ì¢… "ì œëª© - ì¶œì²˜" í˜•ì‹ ì‚¬ìš©
        separators = [' - ', ' â€” ', ' | ', ' Â· ']
        
        for sep in separators:
            if sep in title:
                parts = title.rsplit(sep, 1)
                if len(parts) == 2 and len(parts[1]) < 50:
                    return parts[1].strip(), parts[0].strip()
        
        return None, title

    def extract_source_from_url(self, url):
        """URLì—ì„œ ì¶œì²˜ ì¶”ì¶œ"""
        try:
            # Google ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì²˜ë¦¬
            if 'google.com' in url and 'url=' in url:
                match = re.search(r'url=([^&]+)', url)
                if match:
                    url = unquote(match.group(1))
            
            domain = urlparse(url).netloc
            if domain.startswith('www.'):
                domain = domain[4:]
            
            # ë„ë©”ì¸ì„ ì¶œì²˜ëª…ìœ¼ë¡œ ë³€í™˜
            source_mapping = {
                'chosun.com': 'CHOSUN',
                'donga.com': 'ë™ì•„ì¼ë³´',
                'joongang.co.kr': 'ì¤‘ì•™ì¼ë³´',
                'hankyung.com': 'í•œêµ­ê²½ì œ',
                'mt.co.kr': 'ë¨¸ë‹ˆíˆ¬ë°ì´',
                'ytn.co.kr': 'YTN',
                'sbs.co.kr': 'SBS',
                'mbc.co.kr': 'MBC',
                'kbs.co.kr': 'KBS',
                'reuters.com': 'Reuters',
                'bloomberg.com': 'Bloomberg',
                'cnn.com': 'CNN',
                'bbc.com': 'BBC'
            }
            
            return source_mapping.get(domain, domain.split('.')[0].upper())
            
        except Exception:
            return "Unknown"

    def get_real_url(self, google_url):
        """Google ë¦¬ë‹¤ì´ë ‰íŠ¸ URLì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ"""
        try:
            if 'google.com' in google_url:
                # HEAD ìš”ì²­ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë”°ë¼ê°€ê¸°
                response = requests.head(
                    google_url, 
                    allow_redirects=True, 
                    timeout=10, 
                    verify=False,
                    headers=self.headers
                )
                return response.url
            return google_url
        except Exception:
            return google_url

    def fetch_rss_feed(self, rss_url, max_retries=3):
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨)"""
        for attempt in range(max_retries):
            try:
                # ëœë¤ ì§€ì—°ìœ¼ë¡œ ë´‡ íƒì§€ ë°©ì§€
                time.sleep(random.uniform(1, 3))
                
                response = requests.get(
                    rss_url, 
                    headers=self.headers, 
                    timeout=30, 
                    verify=False
                )
                
                if response.status_code == 200:
                    return response.content
                elif response.status_code == 429:
                    wait_time = (attempt + 1) * 5
                    print(f"429 ì˜¤ë¥˜ ë°œìƒ. {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    print(f"HTTP ì˜¤ë¥˜: {response.status_code}")
                    
            except Exception as e:
                print(f"RSS ìš”ì²­ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(2, 5))
        
        return None

    def crawl_rss_feeds(self, query, max_items=100, time_range='week', language='ko'):
        """ë‹¤ì¤‘ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        print(f"ê²€ìƒ‰ì–´: '{query}'")
        print(f"ì–¸ì–´: {language}")
        print(f"ì‹œê°„ ë²”ìœ„: {time_range}")
        print(f"ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜: {max_items}")
        
        # ì‹œê°„ í•„í„°ë§ì„ ìœ„í•œ cutoff ë‚ ì§œ ê³„ì‚°
        time_filter_days = {
            'day': 1,
            'week': 7,
            'month': 30,
            'year': 365
        }
        
        filter_days = time_filter_days.get(time_range)
        cutoff_date = datetime.now() - timedelta(days=filter_days) if filter_days else None
        
        encoded_query = quote(query)
        
        # ë‹¤ì–‘í•œ RSS í”¼ë“œ URL ìƒì„±
        rss_urls = []
        
        if language == 'ko':
            # í•œêµ­ì–´ RSS í”¼ë“œ
            rss_urls.extend([
                f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko",
                f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR",
                f"https://news.google.com/rss/search?q={encoded_query}+site:chosun.com&hl=ko",
                f"https://news.google.com/rss/search?q={encoded_query}+site:donga.com&hl=ko",
                f"https://news.google.com/rss/search?q={encoded_query}+site:joongang.co.kr&hl=ko"
            ])
        else:
            # ì˜ì–´ RSS í”¼ë“œ
            rss_urls.extend([
                f"https://news.google.com/rss/search?q={encoded_query}&hl=en&gl=US&ceid=US:en",
                f"https://news.google.com/rss/search?q={encoded_query}&hl=en",
                f"https://news.google.com/rss/search?q={encoded_query}+site:reuters.com&hl=en",
                f"https://news.google.com/rss/search?q={encoded_query}+site:bloomberg.com&hl=en",
                f"https://news.google.com/rss/search?q={encoded_query}+site:cnn.com&hl=en"
            ])
        
        items_found = 0
        processed_titles = set()  # ì¤‘ë³µ ì œê±°ìš©
        
        for i, rss_url in enumerate(rss_urls):
            if items_found >= max_items:
                break
                
            print(f"\n[{i+1}/{len(rss_urls)}] RSS í”¼ë“œ ì²˜ë¦¬ ì¤‘...")
            print(f"URL: {rss_url[:80]}...")
            
            content = self.fetch_rss_feed(rss_url)
            if not content:
                continue
            
            try:
                soup = BeautifulSoup(content, 'xml')
                items = soup.find_all('item')
                
                if not items:
                    print("RSS ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                print(f"ë°œê²¬ëœ ì•„ì´í…œ: {len(items)}ê°œ")
                
                for item in items:
                    if items_found >= max_items:
                        break
                    
                    title_tag = item.find('title')
                    link_tag = item.find('link')
                    pub_date_tag = item.find('pubDate')
                    source_tag = item.find('source')
                    description_tag = item.find('description')
                    
                    if not title_tag or not link_tag:
                        continue
                    
                    title_raw = title_tag.text.strip()
                    link_raw = link_tag.text.strip()
                    
                    # ì¤‘ë³µ ì œê±°
                    if title_raw in processed_titles:
                        continue
                    processed_titles.add(title_raw)
                    
                    # ë‚ ì§œ ì²˜ë¦¬ ë° í•„í„°ë§
                    if pub_date_tag:
                        date_parsed = self.parse_rss_date(pub_date_tag.text)
                        
                        # ì‹œê°„ ë²”ìœ„ í•„í„°ë§
                        if cutoff_date:
                            try:
                                item_date = datetime.strptime(date_parsed, '%Y.%m.%d.')
                                if item_date < cutoff_date:
                                    continue
                            except:
                                pass
                    else:
                        date_parsed = datetime.now().strftime('%Y.%m.%d.')
                    
                    # ì¶œì²˜ ì¶”ì¶œ
                    source_name = None
                    cleaned_title = title_raw
                    
                    # 1ìˆœìœ„: source íƒœê·¸
                    if source_tag:
                        source_name = source_tag.text.strip()
                    
                    # 2ìˆœìœ„: ì œëª©ì—ì„œ ì¶”ì¶œ
                    if not source_name:
                        source_from_title, cleaned_title = self.extract_source_from_title(title_raw)
                        if source_from_title:
                            source_name = source_from_title
                    
                    # 3ìˆœìœ„: URLì—ì„œ ì¶”ì¶œ
                    if not source_name:
                        source_name = self.extract_source_from_url(link_raw)
                    
                    # ì‹¤ì œ URL ê°€ì ¸ì˜¤ê¸°
                    real_url = self.get_real_url(link_raw)
                    
                    # ìš”ì•½ ì¶”ì¶œ
                    summary = ""
                    if description_tag:
                        summary = re.sub('<[^<]+?>', '', description_tag.text).strip()
                        summary = summary[:200] + "..." if len(summary) > 200 else summary
                    
                    # ê²°ê³¼ ì €ì¥
                    self.results.append({
                        'ê²€ìƒ‰ í‚¤ì›Œë“œ': query,
                        'ì œëª©': cleaned_title,
                        'URL': real_url,
                        'ë‚ ì§œ': date_parsed,
                        'ì¶œì²˜': source_name,
                        'ìš”ì•½': summary
                    })
                    
                    items_found += 1
                
                print(f"ìˆ˜ì§‘ëœ ì•„ì´í…œ: {items_found}ê°œ")
                
            except Exception as e:
                print(f"RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
        
        return items_found

    def save_to_excel(self, query):
        """ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥"""
        if not self.results:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(self.results)
        
        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=['ì œëª©'], keep='first')
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        if not os.path.exists(self.result_path):
            os.makedirs(self.result_path)
        
        # íŒŒì¼ëª… ìƒì„±
        now = datetime.now()
        filename = f"{now.strftime('%Y-%m-%d %Hì‹œ %Më¶„ %Sì´ˆ')} {query}_RSS.xlsx"
        filepath = os.path.join(self.result_path, filename)
        
        # Excel ì €ì¥
        df.to_excel(filepath, sheet_name='ë‰´ìŠ¤ê²€ìƒ‰ê²°ê³¼', index=False)
        
        print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {filepath}")
        print(f"ì´ {len(df)}ê°œ ê¸°ì‚¬ ì €ì¥ (ì¤‘ë³µ ì œê±° í›„)")
        
        # ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥
        print("\nğŸ“° ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
        print("="*80)
        for i, row in df.head(5).iterrows():
            print(f"{i+1}. {row['ì œëª©'][:60]}...")
            print(f"   ğŸ“… {row['ë‚ ì§œ']} | ğŸ“° {row['ì¶œì²˜']}")
            if row['ìš”ì•½']:
                print(f"   ğŸ’¬ {row['ìš”ì•½'][:80]}...")
            print()
        
        return df

    def crawl(self, query, max_pages=5, time_range='week', language='ko'):
        """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
        print("="*60)
        print("ğŸ” Google News RSS í¬ë¡¤ëŸ¬ ì‹œì‘")
        print("="*60)
        
        # ê²°ê³¼ ì´ˆê¸°í™”
        self.results.clear()
        
        # ìµœëŒ€ ì•„ì´í…œ ìˆ˜ ê³„ì‚° (í˜ì´ì§€ë‹¹ ì•½ 10ê°œ)
        max_items = int(max_pages) * 10
        
        # RSS í¬ë¡¤ë§ ì‹¤í–‰
        items_found = self.crawl_rss_feeds(query, max_items, time_range, language)
        
        if items_found == 0:
            print("\nâŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
            return None
        
        # Excel ì €ì¥
        df = self.save_to_excel(query)
        return df


def main():
    print("="*60)
    print("ğŸš€ Google News RSS í¬ë¡¤ëŸ¬ (429 ì˜¤ë¥˜ ë°©ì§€)")
    print("="*60)
    
    # ì‚¬ìš©ì ì…ë ¥
    query = input("ğŸ” ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    
    max_pages = input("ğŸ“„ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 5): ") or "5"
    
    print("\nâ° ì‹œê°„ ë²”ìœ„ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. í•˜ë£¨")
    print("2. ì¼ì£¼ì¼")
    print("3. í•œë‹¬") 
    print("4. 1ë…„")
    print("5. ì „ì²´")
    
    time_choice = input("ì„ íƒ (ê¸°ë³¸ê°’: 2): ") or "2"
    time_map = {'1': 'day', '2': 'week', '3': 'month', '4': 'year', '5': 'all'}
    time_range = time_map.get(time_choice, 'week')
    
    print("\nğŸŒ ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. í•œêµ­ì–´")
    print("2. ì˜ì–´")
    
    lang_choice = input("ì„ íƒ (ê¸°ë³¸ê°’: 1): ") or "1"
    language = 'ko' if lang_choice == '1' else 'en'
    
    # í¬ë¡¤ëŸ¬ ì‹¤í–‰
    crawler = GoogleNewsRSSCrawler()
    result = crawler.crawl(query, max_pages, time_range, language)
    
    if result is not None:
        print("\nğŸ‰ í¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nğŸ˜ í¬ë¡¤ë§ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()