# ê¸°ì¡´ í¬ë¡¤ë§ ì½”ë“œì— ì›Œë“œ ìƒì„± ê¸°ëŠ¥ì„ ì¶”ê°€í•œ í†µí•© ë²„ì „

from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import requests
import pandas as pd
import re
from urllib.parse import quote
import urllib3
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from docx import Document
from docx.shared import Inches, Cm
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.shared import RGBColor
from PIL import Image
import os
import time
import tempfile

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class IntegratedNewsCrawler:
    def __init__(self):
        self.title_text = []
        self.link_text = []
        self.source_text = []
        self.date_text = []
        self.contents_text = []
        
        # ì›Œë“œ ë¬¸ì„œ ê´€ë ¨
        self.setup_selenium()
        self.doc = Document()
        self.setup_document_style()
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.RESULT_PATH = 'C:/Users/kibae/Desktop/google_news_crawling/result'
        if not os.path.exists(self.RESULT_PATH):
            os.makedirs(self.RESULT_PATH)
    
    def setup_selenium(self):
        """ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--window-size=1200,800')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
    
    def setup_document_style(self):
        """ì›Œë“œ ë¬¸ì„œ ê¸°ë³¸ ìŠ¤íƒ€ì¼ ì„¤ì •"""
        title = self.doc.add_heading('ì–‘ìí†µì‹  ê´€ë ¨ ê¸°ì‚¬', 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        date_para = self.doc.add_paragraph()
        date_para.add_run(f'ê¸°ê°„ : {datetime.now().strftime("%Yë…„ %mì›” %dì¼")}')
        date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        self.doc.add_paragraph()
    
    def date_cleansing(self, date_str):
        """ë‚ ì§œ ì •ì œ í•¨ìˆ˜"""
        now = datetime.now()
        
        try:
            if 'hour' in date_str or 'hr' in date_str:
                hours_ago = int(re.search(r'(\d+)', date_str).group(1))
                target_date = now - timedelta(hours=hours_ago)
                return target_date.strftime('%Y.%m.%d.')
                
            elif 'day' in date_str:
                days_ago = int(re.search(r'(\d+)', date_str).group(1))
                target_date = now - timedelta(days=days_ago)
                return target_date.strftime('%Y.%m.%d.')
                
            elif 'min' in date_str:
                minutes_ago = int(re.search(r'(\d+)', date_str).group(1))
                target_date = now - timedelta(minutes=minutes_ago)
                return target_date.strftime('%Y.%m.%d.')
                
            # ì¶”ê°€ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬...
            
        except Exception as e:
            print(f"Date conversion error: {e} - Input: {date_str}")
        
        return now.strftime('%Y.%m.%d.')
    
    def contents_cleansing(self, contents):
        """ì»¨í…ì¸  ì •ì œ í•¨ìˆ˜"""
        if contents:
            cleaned_content = re.sub('<.+?>', '', str(contents)).strip()
            cleaned_content = re.sub(r'\s+', ' ', cleaned_content)
            self.contents_text.append(cleaned_content)
        else:
            self.contents_text.append("")
    
    def crawler(self, maxpage, query, time_range=None, language='en'):
        """ë‰´ìŠ¤ í¬ë¡¤ë§ (ê¸°ì¡´ ë°©ì‹)"""
        # ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        self.title_text.clear()
        self.link_text.clear()
        self.source_text.clear()
        self.date_text.clear()
        self.contents_text.clear()
        
        encoded_query = quote(query)
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'en-US,en;q=0.9',
        }
        
        time_params = {
            'day': 'd',
            'week': 'w', 
            'month': 'm',
            'year': 'y'
        }
        
        time_param = ""
        if time_range in time_params:
            time_param = f"&tbs=qdr:{time_params[time_range]}"
        
        page = 0
        items_per_page = 10
        max_items = int(maxpage) * items_per_page
        
        while page * items_per_page < max_items:
            url = f"https://www.google.com/search?q={encoded_query}&tbm=nws&hl={language}{time_param}&start={page * items_per_page}"
            
            try:
                response = requests.get(url, headers=headers, timeout=30, verify=False)
                if response.status_code != 200:
                    print(f"Failed to fetch page {page+1}. Status code: {response.status_code}")
                    break
                    
                html = response.text
                soup = BeautifulSoup(html, 'html.parser')
                
                # ë‰´ìŠ¤ ì•„í‹°í´ ì°¾ê¸° (ê¸°ì¡´ ë¡œì§)
                articles = soup.select('div.SoaBEf') or soup.select('div.xuvV6b') or soup.select('div.v7W49e')
                
                if not articles:
                    print(f"No articles found on page {page+1}")
                    break
                
                for article in articles:
                    # ì œëª© ì¶”ì¶œ
                    title_element = (article.select_one('div.MBeuO') or 
                                    article.select_one('h3') or 
                                    article.select_one('.DY5T1d'))
                    
                    if title_element:
                        title = title_element.get_text(strip=True)
                        self.title_text.append(title)
                    else:
                        continue
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_element = article.select_one('a')
                    if link_element and 'href' in link_element.attrs:
                        link = link_element['href']
                        if link.startswith('/url?'):
                            match = re.search(r'url=([^&]+)', link)
                            if match:
                                from urllib.parse import unquote
                                link = unquote(match.group(1))
                        self.link_text.append(link)
                    else:
                        continue
                    
                    # ì¶œì²˜ ì¶”ì¶œ
                    source_element = article.select_one('div.CEMjEf span') or article.select_one('.vN4Yjc')
                    if source_element:
                        source_name = source_element.text.strip()
                        for separator in [' Â· ', ' - ', ' | ']:
                            if separator in source_name:
                                source_name = source_name.split(separator)[0].strip()
                                break
                        self.source_text.append(source_name)
                    else:
                        self.source_text.append("Unknown")
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    date_element = article.select_one('div.OSrXXb span')
                    if date_element:
                        self.date_text.append(self.date_cleansing(date_element.text.strip()))
                    else:
                        self.date_text.append(datetime.now().strftime('%Y.%m.%d.'))
                    
                    # ë‚´ìš© ì¶”ì¶œ
                    content_element = article.select_one('div.GI74Re')
                    if content_element:
                        self.contents_cleansing(content_element.text)
                    else:
                        self.contents_text.append("")
                
                print(f"Processed page {page+1}")
                page += 1
                time.sleep(2)
                
            except Exception as e:
                print(f"Error crawling page {page+1}: {e}")
                break
    
    def capture_screenshot_and_add_to_word(self, url, title, source, category="ì¼ë°˜"):
        """ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜í•˜ê³  ì›Œë“œì— ì¶”ê°€"""
        try:
            print(f"Capturing: {title[:50]}...")
            
            # í˜ì´ì§€ ë¡œë“œ
            self.driver.get(url)
            time.sleep(3)
            
            # ì„ì‹œ ìŠ¤í¬ë¦°ìƒ· íŒŒì¼
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:
                temp_path = temp_file.name
            
            # ìŠ¤í¬ë¦°ìƒ· ì´¬ì˜
            self.driver.save_screenshot(temp_path)
            
            # ì´ë¯¸ì§€ ìµœì í™”
            self.optimize_image(temp_path)
            
            # ì›Œë“œì— ì¶”ê°€
            self.add_to_word_document(title, source, category, url, temp_path)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(temp_path)
            
            return True
            
        except Exception as e:
            print(f"Screenshot error for {title[:30]}: {e}")
            return False
    
    def optimize_image(self, image_path):
        """ì´ë¯¸ì§€ ìµœì í™”"""
        try:
            with Image.open(image_path) as img:
                max_width = 1000
                if img.width > max_width:
                    ratio = max_width / img.width
                    new_height = int(img.height * ratio)
                    img = img.resize((max_width, new_height), Image.Resampling.LANCZOS)
                
                img.save(image_path, 'PNG', optimize=True, quality=85)
        except Exception as e:
            print(f"Image optimization failed: {e}")
    
    def add_to_word_document(self, title, source, category, url, image_path):
        """ì›Œë“œ ë¬¸ì„œì— ë‰´ìŠ¤ ì¶”ê°€"""
        # ì¹´í…Œê³ ë¦¬ë³„ ì´ëª¨ì§€
        category_emoji = {
            "ì–‘ìì»´í“¨í„°": "ğŸ’»",
            "ì–‘ìí†µì‹ ": "ğŸ“¡",
            "ì–‘ìì•”í˜¸": "ğŸ”’", 
            "ì–‘ìì†Œì": "âš›ï¸",
            "ì¼ë°˜": "ğŸ“°"
        }
        
        emoji = category_emoji.get(category, "ğŸ“°")
        
        # ì œëª©
        title_para = self.doc.add_paragraph()
        title_run = title_para.add_run(f"{emoji} {title}")
        title_run.bold = True
        title_run.font.size = Inches(0.16)
        
        # ë©”íƒ€ ì •ë³´
        meta_para = self.doc.add_paragraph()
        meta_text = f"ì¶œì²˜: {source} | ì¹´í…Œê³ ë¦¬: {category} | ìˆ˜ì§‘ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        meta_run = meta_para.add_run(meta_text)
        meta_run.font.color.rgb = RGBColor(128, 128, 128)
        
        # ìŠ¤í¬ë¦°ìƒ·
        try:
            self.doc.add_picture(image_path, width=Inches(6))
            
            # ìº¡ì…˜
            caption_para = self.doc.add_paragraph()
            caption_run = caption_para.add_run(f"â–² {title[:50]}{'...' if len(title) > 50 else ''} (ì¶œì²˜: {source})")
            caption_run.italic = True
            caption_run.font.size = Inches(0.12)
            caption_run.font.color.rgb = RGBColor(100, 100, 100)
            caption_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
        except Exception as e:
            error_para = self.doc.add_paragraph()
            error_run = error_para.add_run(f"[ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {str(e)}]")
            error_run.font.color.rgb = RGBColor(255, 0, 0)
        
        # URL
        url_para = self.doc.add_paragraph()
        url_para.add_run("ì›ë¬¸ ë§í¬: ").bold = True
        url_para.add_run(url)
        
        # êµ¬ë¶„ì„ 
        self.doc.add_paragraph("â”€" * 50)
        self.doc.add_paragraph()
    
    def save_all_results(self, query):
        """ëª¨ë“  ê²°ê³¼ ì €ì¥ (ì—‘ì…€ + ì›Œë“œ)"""
        now = datetime.now()
        timestamp = f'{now.year}-{now.month}-{now.day} {now.hour}ì‹œ {now.minute}ë¶„ {now.second}ì´ˆ'
        
        # 1. ê¸°ì¡´ ë°©ì‹ ì—‘ì…€ ì €ì¥
        min_length = min(len(self.title_text), len(self.link_text), 
                        len(self.source_text), len(self.date_text), len(self.contents_text))
        
        result = {
            "contents": self.contents_text[:min_length],
            "title": self.title_text[:min_length],
            "link": self.link_text[:min_length],
            "date": self.date_text[:min_length],
            "source": self.source_text[:min_length],
        }
        
        df = pd.DataFrame(result)
        excel_filename = f'{timestamp} {query}.xlsx'
        excel_path = os.path.join(self.RESULT_PATH, excel_filename)
        df.to_excel(excel_path, sheet_name='sheet1')
        print(f"ì—‘ì…€ íŒŒì¼ ì €ì¥: {excel_path}")
        
        # 2. ì›Œë“œ ë¬¸ì„œ ì €ì¥
        word_filename = f'{timestamp} {query}_screenshots.docx'
        word_path = os.path.join(self.RESULT_PATH, word_filename)
        self.doc.save(word_path)
        print(f"ì›Œë“œ íŒŒì¼ ì €ì¥: {word_path}")
        
        return df, excel_path, word_path
    
    def run_complete_crawling(self, maxpage, query, time_range=None, language='en', 
                            capture_screenshots=True, max_screenshots=10):
        """ì™„ì „í•œ í¬ë¡¤ë§ ì‹¤í–‰ (í…ìŠ¤íŠ¸ + ìŠ¤í¬ë¦°ìƒ·)"""
        print("=== ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ ===")
        
        # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ í¬ë¡¤ë§
        print("1ë‹¨ê³„: í…ìŠ¤íŠ¸ ë°ì´í„° í¬ë¡¤ë§...")
        self.crawler(maxpage, query, time_range, language)
        
        if not self.title_text:
            print("í¬ë¡¤ë§ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"ì´ {len(self.title_text)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
        
        # 2ë‹¨ê³„: ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ (ì„ íƒì‚¬í•­)
        if capture_screenshots:
            print("2ë‹¨ê³„: ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì¤‘...")
            screenshot_count = 0
            max_capture = min(len(self.title_text), max_screenshots)
            
            for i in range(max_capture):
                if screenshot_count >= max_screenshots:
                    break
                    
                try:
                    # ìœ íš¨í•œ URLì¸ì§€ í™•ì¸
                    url = self.link_text[i]
                    if not url.startswith('http'):
                        continue
                        
                    if self.capture_screenshot_and_add_to_word(
                        url, 
                        self.title_text[i], 
                        self.source_text[i],
                        "ì–‘ìí†µì‹ "  # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬
                    ):
                        screenshot_count += 1
                    
                    time.sleep(2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                    
                except Exception as e:
                    print(f"ìŠ¤í¬ë¦°ìƒ· ì˜¤ë¥˜ {i+1}: {e}")
                    continue
            
            print(f"ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì™„ë£Œ: {screenshot_count}ê°œ")
        
        # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
        print("3ë‹¨ê³„: ê²°ê³¼ ì €ì¥...")
        df, excel_path, word_path = self.save_all_results(query)
        
        print("=== í¬ë¡¤ë§ ì™„ë£Œ ===")
        print(f"ì—‘ì…€ íŒŒì¼: {excel_path}")
        print(f"ì›Œë“œ íŒŒì¼: {word_path}")
        
        return {
            'dataframe': df,
            'excel_path': excel_path, 
            'word_path': word_path,
            'total_news': len(self.title_text),
            'screenshots_captured': screenshot_count if capture_screenshots else 0
        }
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.driver.quit()
        except:
            pass

# ì‚¬ìš© ì˜ˆì‹œ
def main():
    crawler = IntegratedNewsCrawler()
    
    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        result = crawler.run_complete_crawling(
            maxpage=3,              # í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜
            query="ì–‘ìì»´í“¨í„°",       # ê²€ìƒ‰ì–´
            time_range="week",      # ì‹œê°„ ë²”ìœ„ 
            language="ko",          # ì–¸ì–´
            capture_screenshots=True, # ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì—¬ë¶€
            max_screenshots=5       # ìµœëŒ€ ìŠ¤í¬ë¦°ìƒ· ìˆ˜
        )
        
        if result:
            print(f"\nìµœì¢… ê²°ê³¼:")
            print(f"- ì´ ë‰´ìŠ¤: {result['total_news']}ê°œ")
            print(f"- ìŠ¤í¬ë¦°ìƒ·: {result['screenshots_captured']}ê°œ")
            print(f"- ì—‘ì…€ íŒŒì¼: {result['excel_path']}")
            print(f"- ì›Œë“œ íŒŒì¼: {result['word_path']}")
        
    finally:
        crawler.close()

if __name__ == "__main__":
    main()